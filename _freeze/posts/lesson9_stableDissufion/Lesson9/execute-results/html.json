{
  "hash": "0b402cc110f1454139a8879a04d9940d",
  "result": {
    "markdown": "---\ntitle: A different way to look at Stable Diffusion\nauthor: Rekil Prashanth\ntoc: true\nhighlight-style: pygments\nformat:\n  html:\n    code-fold: true\n---\n\nThese are the notes from Lesson 9 of fastai part 2, which is not yet public (may be public by Dec 2022)\n\nThe way stable diffusion is normally explained is focused heavily on a particular mathematical derivation. We've been developing a totally new way of thinking about stable diffusion and that is what we’ll be seeing. It's mathematically equivalent to the approach you'll see in other places but what you'll realize is that this is actually conceptually much simpler and also later in this course we'll be showing you some really innovative directions that this can take you when you think of it in this brand new way. I'm expressing it in a different way and it's equally mathematically valid. \n \n## The magic function \nLet's imagine that we are trying to get something to generate handwritten digits ie stable diffusion for handwritten digits. How do we go about it?\n\nWe're going to start by assuming there's some function or API(black box for now,f- magic function), that takes in a handwritten digit and spits out the probability of it being a handwritten digit. \nFor example  we pass in an image X1 and it spits back p(X1) = 0.98 ie probability that X1 is a handwritten digit, X1 corresponds to the digit 3 in the figure below \n\n<img src='fa_images/func.png' width=\"800\"/>\n\nWhy is this magic function interesting ? \nWe can use this magic function to actually generate handwritten digits. \n\nImage X3 in the figure doesn't look like a digit. Let’s see how we could try to convert it into a handwritten digit. It is a 28x28 image with 784 pixels. \n\n<img src='fa_images/X3.png' width=\"200\"/>\n\nSo let’s slightly alter each of the pixels, and each time we alter a pixel we pass it to the magic function and see how the probability changes. We want to make changes to the image with the hope that the probability value of it being a handwritten digit  increases. \n\n<img src='fa_images/X3updated.png' width=\"800\"/>\n\nLet’s look at a specific example,image X3. Handwritten digits don't normally have any pixels that are black in the very bottom edge(red box), so if we made it a little bit lighter and passed it through our magic function the probability would probably go up a tiny bit(say 0.02 to 0.023). \n\nSo we could do that for every single pixel of the 28x28 image one at a time. We want to find out which pixels we should be making a little bit lighter and which pixels we should be making a little bit darker to make the image look more like a handwritten digit. \n\nPutting this mathematically - we want the gradient of the probability that X3 is a handwritten digit with respect to the pixels of X3 \n\n<img src='fa_images/grad.png' width=\"200\"/>\n\nNote: We didn't say $∂ p(X3)/ ∂ X3$ which you might be familiar with and the reason for that is that we've calculated this $∂ p(X3)/ ∂ X3$  for every single pixel and so when you do it for lots of different inputs you have to turn the ∂ into a ∇ del or a [nabla](https://en.wikipedia.org/wiki/Del).\n\nSo this means that this  $∇p(X3)/ ∇ X3$ has 784 values (28x28 image). They tell us how we can change X3 to make it look more like a digit. We can now change the pixels according to\nthis gradient and this is a lot like what we do when we train neural networks. Except instead of changing the weights in a model we're changing the inputs to the model i.e. the image pixels.\nSo we're going to take every pixel and subtract a little bit of its gradient (c * gradient, where c  can be thought of as a learning rate) to get a new image,X3′, which looks slightly more like a handwritten digit than before.\n\n<img src='fa_images/update.png' width=\"800\"/>\n\nSo now we can pass this new updated image(X3′) through our magic function to calculate a new score and repeat this process.\n\nSo if we have this magic function we can use it to turn any noisy input into something that looks like a handwritten digit(something with a high probability score).\nKey thing to remember: as I change the input pixels I get back a probability score that tells me if this image is a handwritten digit. \n\nSo if we do this by changing each pixel one at a time to calculate a derivative i.e. finite differencing method of calculating derivatives is very slow. Luckily we can use f.backward() and then X3.grad will have the same thing but all in one go by using the analytic derivatives. \nSo now if we have f.backward() and X3.grad we really don’t need the magic function, f. \n\nWe can now multiply the gradient by a small constant c and subtract it from the pixels, we'll do it a few times so that we get larger and larger probabilities of this being a handwritten digit. \n\nSo this magic function will be our neural network which we train to tell us which pixels to change to make an image look more like a handwritten digit.\n\nSo next we need some training data. We create this data by using real handwritten digits and then just chucking random noise on top of it. it's a little bit awkward for us to come up with an exact score which can tell us how much these noisy images are like a handwritten digit so instead let’s predict how much noise was added.\nThis slightly noisy number seven(in the figure below) is actually equal to the original number seven plus some noise. \n\n<img src='fa_images/noise.png' width=\"400\"/>\n\nSo we generate this data and then rather than trying to come up with the arbitrary probability of  predicting how much of a handwritten digit it is we say the amount of noise tells us how much like a digit it is. So something with no noise is very much like a digit(digit 9 in the figure above) and something with lots of noise isn't much like a digit at all(digit 6 in the figure above) .\n\n<img src='fa_images/nn.png' width=\"600\"/>\n\nSo let’s create a neural net for which we need:   \n\n* Inputs -noisy digits   \n* Outputs -  noise   \n* loss function - MSE, between the predicted output(noise) and the actual noise    \n\nWith this we have the ability to know how much do we have to change a pixel by to make it look more like a digit, this is exactly what we wanted - $∇p(X3)/ ∇ X3$ \nSo once we train the neural net we can pass it an image (random noise) and it's\ngoing to spit out information saying which part of that image it think’s is noise and it's going to leave behind the bits that look the most like a digit but it won’t do this in one step. It will do this iteratively, we’ll see why later.\n\n<img src='fa_images/nn_.png' width=\"300\"/>\n\nSo we can repeat this again and again and you can see now why we are doing this multiple times\n\n## Building blocks of stable diffusion\nSo now with this groundwork laid lets see the building blocks of stable diffusion\n\n### 1) UNET \n\n<img src='fa_images/unet.png' width=\"600\"/>\n\nLet’s look at the input and output of the [Unet](https://arxiv.org/abs/1505.04597).   \n\n* Input - somewhat noisy image, it could be no noisy at all or it could be all noise   \n* Output - noise    \nSo if we subtract the output from the input we end up with an approximation of the unnoisy image   \n\nSo our handwritten input image is 28x28, but in reality we would want to generate bigger images. Currently, these models work with 512x512x3 images.  So for training this model we use millions of noisy versions of these 512x512x3 images. It is going to take a long time to train it. How can we make this faster?\nDo we really need to store each and every pixel value? We know pixel values don't change much locally. Can we use this insight? \nFor example a JPEG picture is far fewer bytes than the number of bytes you would get if you multiplied its height by its width by its channels. So the idea is to use compression.\n\n### 2)VAE - Variational autoencoders    \nSo let’s see how to compress it with an autoencoder(AE). \n\n**Architecture of an autoencoder**   \nAt each level we will double the number of channels and use a stride two convolution and at the end we add a few resnet like blocks to squish down the number of channels from 24 to 4.\n\n<img src='fa_images/encoder.png' width=\"800\"/>\n\nSo we started with a 512x512x3 image and we have a representation of this image which has a size of 64x64x4, we have compressed it by  a factor of 48. This representation is called latents. What we just saw is an encoder, We are encoding the “big” image to a much “smaller” representation. \nThis factor of compression makes sense depending on how well we can reconstruct the original image back from these latents of size 64x64x4. \nSo let’s build the inverse process to decode these latents, decoder. And then we can put the encoder and decoder together and train it. \n\n<img src='fa_images/ae_train.png' width=\"800\"/>\n\nWe can use MSE and train this, in the beginning we will get random outputs but later we should get close to our input\n\n<img src='fa_images/ae.png' width=\"800\"/>\n\nSo what is the point of a model that spits back an output that is identical to the input? \n\n<img src='fa_images/enc_dec.png' width=\"800\"/>\n\nThe green bit(when we go from a larger image to a smaller representation) is the encoder and the red bit is the decoder. \nSay I want to send an image to someone, I could pass it through the encoder  and I've now got something that's 48 times smaller than my original picture. The person who\nreceives this can pass it through the decoder(he has a copy of the trained decoder) to get back\nthe original image. \nThis is basically a compression algorithm. \n\nSo how can we use this compression algorithm?  \nWe can pass these “smaller” latents as the input to the Unet instead of passing the “bigger” original images. \n\nSo let us update the inputs and outputs of the Unet: \n\n* Input - somewhat noisy latents   \n* Output - noise    \n\nNow we can subtract the output from the input to get the denoised latents and pass it  to the decoder of theautoencoder to get the best approximation of the denoised version of the image. \n\nThis autoencoder in practice is a Variational Autoencoder. \n\nSo let’s recap what we have done so far. \nWe started with a 512x512x3 image, passed it through the VAE’s encoder to get  a compressed version of size 64x64x4. These latents are then passed through the unet which predicts the noise. We can subtract this noise from the encoder’s latents to get denoised  latents. These denosied latents are passed through the decoder of the VAE to generate an image of size 512x512x3. \n\nFew points to keep in mind:\n\n1) The VAE is an optional building block.  It has the advantage of training the Unet with smaller size latents rather than images, so it’s faster. \n2) The encoder of the VAE is only required during training and not during inference. \n\n### 3)CLIP \nSo next let’s see how the text prompts play a role. \nRather than just feeding in noise and getting back some digit, can we ask it to generate a specific number, say “3”. \n\n<img src='fa_images/classifier.png' width=\"400\"/>\n\nTo achieve this, in addition to passing in the noisy input image, let's also pass in a one hot encoded version of the digit “3”. \n\n<img src='fa_images/classifier1.png' width=\"800\"/>\n\nSo we're now passing two things into this model, the image pixels and what digit it is in one hot encoded vector form. So the model is going to learn how to predict what the noise is\nand since it has this extra information of what the original digit was, we can expect this model to be better at predicting noise than the previous one.\n\nAfter the model has been trained, if we feed in “3”(one hot encoded vector) and the noise, it is going to say the noise is everything that doesn't represent the number three. So this is called guidance.  We can use that guidance to guide the model as to what image we want it to create.\n\nIs one hot encoded vector the best way though? \nSay we want to create an image from the phrase - “a cute teddy”. \nIf we were to use 1- hot encoded vectors then we have to create a 1-hot encoded vector for every phrase, which seems very inefficient. \n\nWe’ll create a  model that can use the phrase -”a cute teddy” as an input  and can output a vector of numbers,embeddings, that in some way represents what “cute teddies” look like. \n\nSo we can gather images from the internet and if they have alt tags they will have some description of the image i.e. a text associated with that image.   \n\n<img src='fa_images/input_clip.png' width=\"800\"/>\n\nNow we can create two models, one model which is a text encoder and one model which is an image encoder.\n\n<img src='fa_images/clip.png' width=\"800\"/>\n\nSo we can pass the image to the image encoder and text to the text encoder and they will each give us two embeddings.\n\nNow when we pass the image of the swan, through our image model we would like it to return embeddings which are similar to the embeddings that we get when we pass the text “the graceful swan” through the text encoder. In other words we want their embeddings to be similar. \nHow do we tell our model to do this? We can use dot product to check for similarity between the embeddings. Higher the dot product more similar are the embeddings. \n\n<img src='fa_images/dot.png' width=\"600\"/>\n\nSo now we have a grid of images and text, each combination of their embeddings will give us a score when we take their dot product. We want the dot product for only the matching image-text pairs to be high(blue,diagonal element) and similarly we want the non matching pairs of text and image to be small(red, off diagonal elements)\n\n<img src='fa_images/clip1.png' width=\"400\"/>\n\nSo our loss function can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.\n\n<img src='fa_images/loss_clip.png' width=\"400\"/>\n\nIf we want this loss function to be good then we're going to need the weights of our model\nfor the text encoder to spit out embeddings that are very similar to the image embeddings that they're paired with and not similar to the embedding of the images they are not paired with.\n\nNow we can feed our text encoder with “a graceful swan”, “some beautiful swan”,\n“such a lovely swan” and these should all give very similar embeddings because these would all\nrepresent very similar images of swans. \n\nWe've successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model.\n\nSo we took this detour because creating 1-hot encoded vectors for all the possible phrases was impractical. We can can now take our phrase - “a cute teddy bear” and feed it in text encoder to get out some features/embeddings.\n\n<img src='fa_images/clip2.png' width=\"800\"/>\n\nThese features are what we will use as guides instead of the 1- hot encoded vectors when we train our Unet. So we pass the phrase - “a cute teddy’ to our text encoder, which will generate embeddings which is going to be used as a guide by our model to turn the input noisy image into something that is similar to things that it's previously seen that are “cute teddies”\n\nThis pair of models have a name - CLIP,Contrastive Language-Image Pre-training and the loss we are using is called contrastive loss. \n\nLet us see what all building blocks we have so far.\n\n<img src='fa_images/summ.png' width=\"400\"/>\n\n* we've got a Unet that can denoise latents into unnoisy latents \n* we've got the decoder of VAE that can take latents and create an image\n* we've got the CLIP text encoder which can guide the Unet with captions\n\nStable diffusion is a latent diffusion model and what that means is that it doesn't operate in the pixel space, it operates on in the latent space of some other autoencoder model and in this case that is a variational autoencoder. \n\n**Some jargon:**\n\n* score function \n* time-steps.\n\n<img src='fa_images/score.png' width=\"800\"/>\n\nThese gradients that we have are often called the score function.\n\n“Time steps” is the jargon used in a lot of papers but we never used any time steps during our training. This is basically an overhang from how the math was formulated in the initial papers. We will avoid using the term time steps but we can see what time steps are though it's got nothing to do with time in real life.\n\nWe added varying levels of noise to our images, some were very noisy, some were not noisy at all, some had no noise and some would have been pure noise.\n\nLet’s create a noising schedule,which is a monotonically decreasing function. Where say the x-axis(“t”) varies from one to a thousand. Now we randomly pick a number between one and thousand, we look up in the noise schedule and return the sigma(or beta is what you’ll see in papers).  Say we happen to pick the number four, we would look up to find a value on the y-axis which is the sigma, the amount of noise to add to your image if you randomly picked a four. \n\n<img src='fa_images/noise_Sched.png' width=\"400\"/>\n\nIf you randomly pick one you're going to have a lot of noise and if you pick a 1000 you're going to have hardly any noise. \n\nSo when we are training we need to pick some random amount of noise for every image. One way to get the random noise is to pick a random number between one to a thousand, look it up on this noise scheduler function and that will tell us the sigma of the noise to be added.\n\nPeople refer to this t as the time step,  nowadays you don't really have to look up in the noise scheduler. A lot of people are starting to get rid of this idea altogether and some people instead will simply say how much noise was there.\n\nSo each time you create a mini batch during training, you randomly pick a batch of images from your training set, you randomly pick either an amount of noise or you randomly pick a t and then look up the amount of noise and then use that amount of noise to create the noisy images. Then you pass that mini batch into your model to train it and that trains the weights in your model so it can learn to predict noise.\n\n<img src='fa_images/batch.png' width=\"400\"/>\n\nHow exactly do we do this inference process ?\nWhen you're generating a picture from pure noise, this corresponds to t=0 on the noise scheduler where you have maximum noise. So you want it to learn to remove noise but if you do this in one step you’ll end up with bad images. \n\n<img src='fa_images/step1.png' width=\"400\"/>\n\nSo  in practice we get the prediction of the noise and then we multiply it by some constant,c, which is like a learning rate but here we're not updating weights now we're updating pixels and we subtract it from the original noisy pixels.  So it doesn’t actually predict the final denoised image, all it does is remove some small factor of the  noise to give us a slightly denoised image.\n\nThe reason we don't jump all the way to the final image is because things that look like the image we got by using t=1 (crappy image) never appeared in our training set(does this mean we never train with highly noised images??)  and so since it never appeared in our training set our model has no idea what to do with it. Our model only knows how to deal with things that look like somewhat noisy latents and so that's why we subtract just a small factor of the noise so that we still have a somewhat noisy latent for this process to repeat a bunch of times. \n\nThe questions of what c to use and how do we go from the prediction of noise to what we subtract are the kind of the things that you decide in the actual diffusion sampler.\nThe sampler is used to both add the noise and how to subtract the noise \n\nThis looks a lot like deep learning optimizers.\nIf you change the same parameters by a similar amount multiple times in multiple steps maybe\nyou should increase the amount you change them, this concept is something we call Momentum. \nWe even have better ways of doing that where we say well what happens as the variance changes. Maybe we can look at that as well and that gives us something called Adam and these are types of optimizers.\nAll diffusion-based models came from a very different world of math, which is the world of differential equations. There are a lot of parallel concepts in these two worlds. Differential equations is all about how to take bigger steps instead of taking smaller steps so we can converge quicker.  Differential equation solvers use a lot of the same kind of ideas as optimizers, if you squint.  One thing that differential equations solvers do which is that they tend to take t as an input and in fact pretty much all diffusion models do that  too, we hadn’t spoken about that.\n\n<img src='fa_images/t.png' width=\"600\"/>\n\nPretty much all diffusion models don't just take the input pixels and the caption, they also take t. \nAnd the idea is that the model will be better at removing the noise if you tell it how much noise there is and remember t is related to how much noise there is.\n\nJeremy very strongly suspects that this premise is incorrect because for a fancy neural net figuring out how noisy something is very simple. So when you don’t need to pass in t things stop looking like differential equations and they start looking more like optimizers\n\nWe could swap MSE with perceptual loss. \nAll these things suddenly become possible when we start thinking of this as an optimization problem rather than a differential equation solving problem\n\n",
    "supporting": [
      "Lesson9_files"
    ],
    "filters": [],
    "includes": {}
  }
}